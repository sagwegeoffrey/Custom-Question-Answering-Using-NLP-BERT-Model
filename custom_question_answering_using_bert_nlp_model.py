# -*- coding: utf-8 -*-
"""Custom Question Answering using BERT NLP Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pW3l6T-yKmQ_1713f2BaMXoSzehc3FAc
"""

!pip install simpletransformers

#!pip install -U adapter-transformers
#!pip install datasets

# Loading the data and converting into the required format
import json

with open('/content/train-v2.0.json', 'r') as f:
  train_data = json.load(f)

train_data

with open(r"dev-v2.0.json", "r") as read_file:
     test_data = json.load(read_file)

test_data

import logging
#import torch
from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs

model_type="bert"
model_name= "bert-base-cased"
if model_type == "bert":
    model_name = "bert-base-cased"

elif model_type == "roberta":
    model_name = "roberta-base"

elif model_type == "distilbert":
    model_name = "distilbert-base-cased"

elif model_type == "distilroberta":
    model_type = "roberta"
    model_name = "distilroberta-base"

elif model_type == "electra-base":
    model_type = "electra"
    model_name = "google/electra-base-discriminator"

elif model_type == "electra-small":
    model_type = "electra"
    model_name = "google/electra-small-discriminator"

elif model_type == "xlnet":
    model_name = "xlnet-base-cased"

# Configure the model 
model_args = QuestionAnsweringArgs()
model_args.train_batch_size = 16
model_args.evaluate_during_training = True
model_args.n_best_size=3
model_args.num_train_epochs=10

#!pip install wandb

### Advanced Methodology
train_args = {
    "reprocess_input_data": True,
    "overwrite_output_dir": True,
    "use_cached_eval_features": True,
    "output_dir": f"outputs/{model_type}",
    "best_model_dir": f"outputs/{model_type}/best_model",
    "evaluate_during_training": True,
    "max_seq_length": 128,
    "num_train_epochs": 10,
    "evaluate_during_training_steps": 1000,
    "wandb_project": "Question Answer Application",
    "wandb_kwargs": {"name": model_name},
    "save_model_every_epoch": False,
    "save_eval_checkpoints": False,
    "n_best_size":3,
    # "use_early_stopping": True,
    # "early_stopping_metric": "mcc",
    # "n_gpu": 2,
    # "manual_seed": 4,
    # "use_multiprocessing": False,
    "train_batch_size": 128,
    "eval_batch_size": 64,
    # "config": {
    #     "output_hidden_states": True
    # }
}

model = QuestionAnsweringModel(
    model_type,model_name, args=train_args
)

### Remove output folder
!rm -rf output

from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Train the model
model.train_model(test_data, eval_data=train_data)
#model.save_weights('/content/train.json/savefile')

#from transformers import BertForSequenceClassification

#model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
#model.save_pretrained('results/tokenizer/')
#model.compile()
#model.load_weights('/content/train/savefile')

#print("model created")

# Evaluate the model
result, texts = model.eval_model(test_data)

texts

import collections
from simpletransformers.question_answering.question_answering_utils import get_tokens
def compute_f1(test_data, train_data):
  gold_toks = get_tokens(test_data)
  pred_toks = get_tokens(train_data)
  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)
  num_same = sum(common.values())
  if len(gold_toks) == 0 or len(pred_toks) == 0:
    return int(gold_toks == pred_toks)
  if num_same == 0:
    return 0
  precision = 1.0 * num_same / len(pred_toks)
  recall = 1.0 * num_same / len(gold_toks)
  f1 = (2 * precision * recall) / (precision + recall)
  return f1

print(result)

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

from simpletransformers.question_answering.question_answering_utils import get_tokens
import sklearn

model = QuestionAnsweringModel(
    model_type,model_name, args=train_args
)

model.eval_model(train_data, eval_accuracy= sklearn.metrics.accuracy_score)
model.eval_model(test_data, eval_accuracy=sklearn.metrics.accuracy_score)
#model.eval_model(test_data, f1_score=sklearn.metrics.f1_score)

# Make predictions with the model
to_predict = [
   {
       "context": "Rangi nyeupe ni alama ya Amani, Kumwagika kwa damu wakati wa kupigania uhuru kunaonyeshwa kwa rangi Nyekundu, Rangi iliyo juu katika bendera ya Kenya huwa Nyeusi.", 
       "qas": [
           {
              "question": "Rangi nyeupe ni alama ya?", 
              "id": "0"
           }
        ],
     }
]

answers, probabilities = model.predict(to_predict)
print(answers)

preds = model.predict(test_data[:10])

preds

train_data[10]

!pip install dump

import os

os.makedirs('results', exist_ok=True)

submission = {i['id']: i['answer'] for pred in preds for i in pred for key, value in i.items() if key == "answer"}

with open('results/submission.json', 'w') as f:
  json.dump(submission, f)

from keras import models

batch_size = 50

# Create a TransformerModel
#model =  QuestionAnsweringModel('bert', 'bert-base-cased') #, args=train_args, use_cuda=False)
#print(train_data(10))

# Train the model
#model.train_model(train_data, eval_model=test_data)

# Evaluate the model
#result, model_outputs, predictions = model.eval_model(train_data, acc=sklearn.metrics.accuracy_score)

#predictions, raw_outputs = model.predict(["Rangi nyeupe ni alama ya?' â€” Rangi nyeupe ni alama ya Amani?" * 10])
#print(predictions)
#print(raw_outputs)

import Trainer

# initialize Trainer object
trainer = Trainer(
    model,
    training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    data_collator=default_data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# loading the dataset
datasets=load_dataset('imdb')

# instantiate the tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=True)

# tokenize the texts
# refer notebook for the preprocess function
datasets = datasets.map(preprocess_function, 
                        batched=True, load_from_cache_file=True)

# load pre-trained BERT model
model = AutoModelForSequenceClassification.from_pretrained(
        model_name_or_path,
        num_labels=len(label_list))

args = TrainingArguments(
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=1,
    weight_decay=0.01,
    output_dir='/tmp/cls'
)

trainer = Trainer(model,
    args,
    train_dataset=datasets["train"],
    eval_dataset=datasets["test"],
    data_collator=default_data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# start training
trainer.train()

# loading the dataset
# datasets=load_dataset('imdb')

# instantiate the tokenizer
#tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=True)

# tokenize the texts
# refer notebook for the preprocess function
datasets = datasets.map(preprocess_function, batched=True, load_from_cache_file=True)

# load pre-trained BERT model
#from transformers import AutoTokenizer, AutoConfig 
args = QuestionAnsweringArgs(
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=1,
    weight_decay=0.01,
    output_dir='/tmp/cls'
)

trainer = Trainer(model,
    args,
    train_dataset=datasets["train_data"],
    eval_dataset=datasets["test_data"],
    data_collator=default_data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# start training
trainer.train()

#import os
#os.makedirs('results', exist_ok=True)

#submission = {i['id']: i['answer'] for text in texts for key, value i.items() if key =="answer"}

#with open('results/submission.json', 'w') as f:
  #json.dumb(submission, f)

from simpletransformers.question_answering import QuestionAnsweringModel

train_args = {
    'learning_rate': 3e-5,
    'num_train_epochs': 3,
    'max_seq_length': 384,
    'doc_stride': 128,
    'overwrite_output_dir': True,
    'reprocess_input_data': False,
    'train_batch_size': 6,
    'gradient_accumulation_steps': 8,
}

model = QuestionAnsweringModel('bert', 'bert-base-uncased', args = train_args)

from google.colab import files
files.download("/content/out.zip")

#model.train_model(train_data[:0.4])
import torch
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

# Load the fine-tuned model
tokenizer = AutoTokenizer.from_pretrained("/content/outputs/bert/best_model")
model = AutoModelForQuestionAnswering.from_pretrained("/content/outputs/bert/best_model")


question = "Nani aliwabeba Njeri hadi Nakuru?"

context = """Babu alitubeba kwa gari lake hadi mji wa Nakuru. Njiani tuliwaona swara, pundamilia na nyani wakila nyasi mwituni. Babu aliahidi kunipeleka katika ziwa la Nakuru kuwaona wanyama zaidi.Niliona miti mingi ambayo sikuweza kuiona Australia. Ilikuwa mirefu na yenye miiba. Babu alinieleza inaitwa mishita."""


# 1. TOKENIZE THE INPUT
# note: if you don't include return_tensors='pt' you'll get a list of lists which is easier for 
# exploration but you cannot feed that into a model. 
inputs = tokenizer.encode_plus(question, context, return_tensors="pt") 

# 2. OBTAIN MODEL SCORES
# the AutoModelForQuestionAnswering class includes a span predictor on top of the model. 
# the model returns answer start and end scores for each word in the text
answer_start_scores, answer_end_scores = model(**inputs,return_dict=False)
answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score
answer_end = torch.argmax(answer_end_scores) +1 # get the most likely end of answer with the argmax of the score




# 3. GET THE ANSWER SPAN
# once we have the most likely start and end tokens, we grab all the tokens between them
# and convert tokens back to words!
tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end]))

"""# New Section"""

def question_answer(question, text):
    
    #tokenize question and text as a pair
    input_ids = tokenizer.encode(question, text)
    
    #string version of tokenized ids
    tokens = tokenizer.convert_ids_to_tokens(input_ids)
    
    #segment IDs
    #first occurence of [SEP] token
    sep_idx = input_ids.index(tokenizer.sep_token_id)
    #number of tokens in segment A (question)
    num_seg_a = sep_idx+1
    #number of tokens in segment B (text)
    num_seg_b = len(input_ids) - num_seg_a
    
    #list of 0s and 1s for segment embeddings
    segment_ids = [0]*num_seg_a + [1]*num_seg_b
    assert len(segment_ids) == len(input_ids)
    
    #model output using input_ids and segment_ids
    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))
    
    #reconstructing the answer
    answer_start = torch.argmax(output.start_logits)
    answer_end = torch.argmax(output.end_logits)
    if answer_end >= answer_start:
        answer = tokens[answer_start]
        for i in range(answer_start+1, answer_end+1):
            if tokens[i][0:2] == "##":
                answer += tokens[i][2:]
            else:
                answer += " " + tokens[i]
                
    if answer.startswith("[CLS]"):
        answer = "Unable to find the answer to your question."
    
    print("\nPredicted answer:\n{}".format(answer.capitalize()))

question = "Waliona wanyama wagani njiani wakienda Nakuru?"

text = """Tulipofika kwenye lango la ziwa la Nakuru tuliwaona ngiri, pundamilia na nyani. Tulilipa ada na kufunguliwa lango kisha tukatumbukia ndani ya msitu."""

question_answer(question, text)